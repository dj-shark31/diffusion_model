Metadata-Version: 2.4
Name: diffusion_model
Version: 0.1.0
Summary: A PyTorch implementation of DDPM (Denoising Diffusion Probabilistic Models) for image generation
Home-page: https://github.com/yourusername/diffusion-model
Author: David Jany
Author-email: djany31@gmail.com
Project-URL: Bug Reports, https://github.com/yourusername/diffusion-model/issues
Project-URL: Source, https://github.com/yourusername/diffusion-model
Project-URL: Documentation, https://github.com/yourusername/diffusion-model#readme
Keywords: diffusion,ddpm,generative,pytorch,machine-learning,deep-learning,image-generation,ai,artificial-intelligence
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Image Processing
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=1.9.0
Requires-Dist: torchvision>=0.10.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: matplotlib>=3.3.0
Requires-Dist: tqdm>=4.62.0
Requires-Dist: scipy>=1.7.0
Requires-Dist: scikit-learn>=1.0.0
Requires-Dist: lpips>=0.1.4
Requires-Dist: Pillow>=8.3.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: black>=21.0; extra == "dev"
Requires-Dist: flake8>=3.8; extra == "dev"
Requires-Dist: isort>=5.0; extra == "dev"
Requires-Dist: mypy>=0.800; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=4.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0; extra == "docs"
Requires-Dist: myst-parser>=0.15; extra == "docs"
Provides-Extra: gpu
Requires-Dist: torch>=1.9.0; extra == "gpu"
Provides-Extra: full
Requires-Dist: tensorboard>=2.7.0; extra == "full"
Requires-Dist: wandb>=0.12.0; extra == "full"
Requires-Dist: pytorch-fid>=0.2.0; extra == "full"
Requires-Dist: lpips>=0.1.4; extra == "full"
Requires-Dist: scikit-image>=0.19.0; extra == "full"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# DDPM on MNIST

A pure implementation of Denoising Diffusion Probabilistic Models (DDPM) on the MNIST dataset using PyTorch.

## Overview

This project implements a complete DDPM pipeline with the following features:

- **UNet-mini architecture** without attention layers
- **ε-prediction** objective
- **T=1000** timesteps with linear β schedule
- **Multiple samplers**: Ancestral, Deterministic, and DDIM
- **Comprehensive evaluation**: FID, Inception Score, LPIPS, PSNR
- **Training utilities**: EMA, gradient clipping, checkpointing

## Repository Structure

```
diffusion/
├── model.py          # UNet architecture and time embeddings
├── schedulers.py     # Beta/alpha/sigma schedules, DDIM/EDM
├── losses.py         # Epsilon vs x0 objectives
├── sampler.py        # Ancestral & deterministic samplers
├── conditioning.py   # Class labels, masks, text encoding
├── train.py          # Training loops, EMA, logging
├── sample.py         # Load checkpoint, generate grids
├── utils.py          # EMA, gradient clip, seeding
└── eval.py           # FID/IS/LPIPS/PSNR evaluation

configs/
└── default_config.py # Default configuration

data/                 # Dataset storage
```

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd diffusion_model
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

### Training

Train a DDPM model on MNIST:

```bash
cd diffusion
python train.py --epochs 100 --batch-size 128 --lr 1e-4
```

Resume training from a checkpoint:

```bash
python train.py --resume checkpoints/checkpoint_epoch_50.pth
```

### Sampling

Generate samples from a trained model:

```bash
python sample.py --checkpoint checkpoints/best_model.pth --num-samples 16
```

Generate samples with different samplers:

```bash
# Deterministic sampling (fast)
python sample.py --checkpoint checkpoints/best_model.pth --sampler deterministic --steps 50

# DDIM sampling
python sample.py --checkpoint checkpoints/best_model.pth --sampler ddim --steps 50 --eta 0.0

# Ancestral sampling (slow but high quality)
python sample.py --checkpoint checkpoints/best_model.pth --sampler ancestral --steps 1000
```

Generate comparison samples:

```bash
python sample.py --checkpoint checkpoints/best_model.pth --comparison
```

Generate progressive samples:

```bash
python sample.py --checkpoint checkpoints/best_model.pth --progressive
```

### Evaluation

Evaluate model performance:

```bash
python eval.py --checkpoint checkpoints/best_model.pth --num-samples 1000
```

## Configuration

The default configuration is in `configs/default_config.py`. Key parameters:

### Model Parameters
- `image_size`: 28 (MNIST image size)
- `model_channels`: 64 (base channel width)
- `time_emb_dim`: 128 (time embedding dimension)

### Training Parameters
- `num_epochs`: 100
- `batch_size`: 128
- `learning_rate`: 1e-4
- `ema_decay`: 0.9999

### Diffusion Parameters
- `num_train_timesteps`: 1000
- `beta_start`: 0.0001
- `beta_end`: 0.02
- `beta_schedule`: 'linear'

## Model Architecture

The UNet-mini architecture consists of:

1. **Time Embedding**: Sinusoidal position embeddings + MLP
2. **Downsampling**: 3 blocks with skip connections
3. **Bottleneck**: 2 conv layers with group norm
4. **Upsampling**: 3 blocks with skip connections
5. **Output**: 1x1 conv for noise prediction

No attention layers are used, making it a "mini" version suitable for MNIST.

## Training Details

- **Loss**: MSE loss on predicted noise (ε-prediction)
- **Optimizer**: AdamW with weight decay
- **Scheduler**: Cosine annealing learning rate
- **Regularization**: EMA with decay 0.9999
- **Gradient Clipping**: Max norm 1.0

## Sampling Methods

1. **Ancestral Sampling**: Standard DDPM sampling (T=1000 steps)
2. **Deterministic Sampling**: DDPM with η=0 (deterministic)
3. **DDIM Sampling**: Fast deterministic sampling with fewer steps

## Evaluation Metrics

- **FID**: Fréchet Inception Distance
- **Inception Score**: Measures image quality and diversity
- **LPIPS**: Learned Perceptual Image Patch Similarity
- **PSNR**: Peak Signal-to-Noise Ratio

## Results

Expected results on MNIST after 100 epochs:
- FID: ~5-10
- Inception Score: ~8-9
- LPIPS: ~0.1-0.2
- PSNR: ~20-25 dB

## Tips

1. **Start with fewer epochs** (10-20) to test the setup
2. **Use deterministic sampling** for quick evaluation during training
3. **Monitor training logs** in the `logs/` directory
4. **Save checkpoints regularly** to resume training if needed
5. **Use EMA model** for final evaluation and sampling

## Troubleshooting

### Common Issues

1. **Out of Memory**: Reduce batch size or model channels
2. **Slow Training**: Use fewer workers or smaller model
3. **Poor Quality**: Increase training epochs or adjust learning rate
4. **Import Errors**: Install missing dependencies from requirements.txt

### Performance Tips

1. **Use GPU** for faster training
2. **Increase batch size** if memory allows
3. **Use mixed precision** for faster training (not implemented)
4. **Parallel sampling** for faster evaluation

## License

This project is licensed under the MIT License.

## Citation

If you use this code, please cite the original DDPM paper:

```bibtex
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}
``` 
